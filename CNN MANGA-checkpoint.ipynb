{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import time\n",
    "import datetime\n",
    "import itertools\n",
    "import shutil\n",
    "from math import ceil\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import color\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout \n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling2D, UpSampling2D\n",
    "from keras.layers import DepthwiseConv2D, BatchNormalization, LeakyReLU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing import image\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5)\n",
    "\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform dowload folders into jpg in the Downloads directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Downloads ='datasets/Downloads/'\n",
    "datasets = 'datasets/'\n",
    "\n",
    "dossierlist=os.listdir(Downloads)\n",
    "for dossier in dossierlist:\n",
    "    \n",
    "    fichierlist = os.listdir(Downloads + dossier)\n",
    "    for fichier in fichierlist:\n",
    "        \n",
    "        src = Downloads + dossier + '/' + fichier \n",
    "        \n",
    "        filename = dossier[18:] + '_' + fichier\n",
    "        dst = datasets + filename\n",
    "        \n",
    "        os.rename(src, dst)\n",
    "    \n",
    "    os.rmdir(Downloads + dossier)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform jpg into slices and store in Color and Gray directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop image in order to get more data in dataset and less NN to train\n",
    "# We want to get a lot of image of 256 by 256\n",
    "def crop_and_resize(path, fichier, height, width):\n",
    "    \n",
    "    image = cv2.imread(path + fichier)\n",
    "    maxH, maxW, alpha = image.shape\n",
    "    filename = os.path.splitext(fichier)[0]\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(0,maxH,height):\n",
    "        \n",
    "        for j in range(0,maxW,width):\n",
    "            \n",
    "            try:\n",
    "                crop = image[i : (i+height), j : (j+width)] \n",
    "                resize = cv2.resize(crop, (512, 512), cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(path + \"Images/\" + filename + \"_ \"+ str(k) + \".jpg\", resize)\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"error! \" + str(fichier) + ' i : ' + str(i) + ' j : ' + str(j))\n",
    "                print(str(e))\n",
    "                pass\n",
    "            k +=1\n",
    "            \n",
    "    os.remove(path + fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we want A lot of Gray and color image of (1000, 1000)px\n",
    "import os\n",
    "filelist=os.listdir('datasets/')\n",
    "for fichier in filelist:\n",
    "    if fichier.endswith('.jpg'):\n",
    "        crop_and_resize(\"datasets/\", fichier, 1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 32] Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus: 'datasets/Images/The_Gamer_v04_c247_01_ 9.jpg' -> 'datasets/Images/Done/The_Gamer_v04_c247_01_ 9.jpg'\n"
     ]
    }
   ],
   "source": [
    "Images = []\n",
    "\n",
    "cpt = 0\n",
    "\n",
    "for filename in os.listdir('datasets/Images/'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        try : \n",
    "\n",
    "            # Read the image\n",
    "            image = cv2.imread('datasets/Images/'+filename)\n",
    "            Images.append(image)\n",
    "            \n",
    "            # Move the image\n",
    "            os.rename('datasets/Images/'+filename, 'datasets/Images/Done/'+filename)\n",
    "            \n",
    "            cpt = cpt + 1\n",
    "            if cpt == 2000:\n",
    "                break\n",
    "                     \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "\n",
    "Images = np.array(Images, dtype=np.float16)\n",
    "\n",
    "# Set up training and test data\n",
    "split = int(0.9*len(Images))\n",
    "Xtrain = Images[:split]\n",
    "Xvalid = Images[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 10\n",
    "def image_a_b_gen(batch_size, Images):\n",
    "    for batch in datagen.flow(Images, batch_size=batch_size):\n",
    "        \n",
    "        gray = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in batch]\n",
    "        gray = np.array(gray)\n",
    "        gray = gray.reshape(gray.shape+(1,))\n",
    "        X = gray / 255\n",
    "        \n",
    "        Y = batch / 255\n",
    "\n",
    "        yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ncharbit\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 256, 256, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 128)       295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 256, 256, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 256, 256, 32)      18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 512, 512, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 512, 512, 3)       867       \n",
      "=================================================================\n",
      "Total params: 3,892,483\n",
      "Trainable params: 3,892,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the neural network\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(512, 512, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(3, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 8374s 47s/step - loss: 92.4929 - val_loss: 0.1565\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.15650, saving model to weights-improvement-01-0.1565.hdf5\n",
      "Epoch 2/100\n",
      "180/180 [==============================] - 8445s 47s/step - loss: 0.0649 - val_loss: 0.0196\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.15650 to 0.01958, saving model to weights-improvement-02-0.0196.hdf5\n",
      "Epoch 3/100\n",
      "180/180 [==============================] - 8385s 47s/step - loss: 0.0325 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.01958 to 0.01350, saving model to weights-improvement-03-0.0135.hdf5\n",
      "Epoch 4/100\n",
      "180/180 [==============================] - 8433s 47s/step - loss: 0.0556 - val_loss: 0.0147\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.01350\n",
      "Epoch 5/100\n",
      "180/180 [==============================] - 8401s 47s/step - loss: 0.0280 - val_loss: 0.0158\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01350\n",
      "Epoch 6/100\n",
      "180/180 [==============================] - 8279s 46s/step - loss: 0.0161 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.01350 to 0.01159, saving model to weights-improvement-06-0.0116.hdf5\n",
      "Epoch 7/100\n",
      "180/180 [==============================] - 8233s 46s/step - loss: 0.0144 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.01159 to 0.00684, saving model to weights-improvement-07-0.0068.hdf5\n",
      "Epoch 8/100\n",
      "180/180 [==============================] - 8356s 46s/step - loss: 0.0119 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00684 to 0.00684, saving model to weights-improvement-08-0.0068.hdf5\n",
      "Epoch 9/100\n",
      "180/180 [==============================] - 8423s 47s/step - loss: 0.0117 - val_loss: 0.0091\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00684\n",
      "Epoch 10/100\n",
      "180/180 [==============================] - 8431s 47s/step - loss: 0.0112 - val_loss: 0.0076\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00684\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/100\n",
      "180/180 [==============================] - 8437s 47s/step - loss: 0.0074 - val_loss: 0.0057\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00684 to 0.00568, saving model to weights-improvement-11-0.0057.hdf5\n",
      "Epoch 12/100\n",
      "180/180 [==============================] - 8450s 47s/step - loss: 0.0068 - val_loss: 0.0055\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00568 to 0.00546, saving model to weights-improvement-12-0.0055.hdf5\n",
      "Epoch 13/100\n",
      "180/180 [==============================] - 8464s 47s/step - loss: 0.0066 - val_loss: 0.0054\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00546 to 0.00543, saving model to weights-improvement-13-0.0054.hdf5\n",
      "Epoch 14/100\n",
      "180/180 [==============================] - 8448s 47s/step - loss: 0.0064 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00543 to 0.00532, saving model to weights-improvement-14-0.0053.hdf5\n",
      "Epoch 15/100\n",
      "180/180 [==============================] - 8277s 46s/step - loss: 0.0063 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00532 to 0.00501, saving model to weights-improvement-15-0.0050.hdf5\n",
      "Epoch 16/100\n",
      "180/180 [==============================] - 8289s 46s/step - loss: 0.0062 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00501\n",
      "Epoch 17/100\n",
      "180/180 [==============================] - 8335s 46s/step - loss: 0.0062 - val_loss: 0.0052\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00501\n",
      "Epoch 18/100\n",
      "180/180 [==============================] - 8415s 47s/step - loss: 0.0061 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00501 to 0.00495, saving model to weights-improvement-18-0.0050.hdf5\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 19/100\n",
      "180/180 [==============================] - 8462s 47s/step - loss: 0.0059 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00495 to 0.00495, saving model to weights-improvement-19-0.0049.hdf5\n",
      "Epoch 20/100\n",
      "180/180 [==============================] - 8457s 47s/step - loss: 0.0059 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.00495 to 0.00490, saving model to weights-improvement-20-0.0049.hdf5\n",
      "Epoch 21/100\n",
      "180/180 [==============================] - 8433s 47s/step - loss: 0.0059 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00490\n",
      "Epoch 22/100\n",
      "180/180 [==============================] - 8484s 47s/step - loss: 0.0059 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00490\n",
      "Epoch 23/100\n",
      "180/180 [==============================] - 8479s 47s/step - loss: 0.0058 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00490\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 24/100\n",
      "180/180 [==============================] - 8447s 47s/step - loss: 0.0059 - val_loss: 0.0050\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00490\n",
      "Epoch 25/100\n",
      "180/180 [==============================] - 8399s 47s/step - loss: 0.0058 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.00490 to 0.00488, saving model to weights-improvement-25-0.0049.hdf5\n",
      "Epoch 26/100\n",
      "180/180 [==============================] - 8383s 47s/step - loss: 0.0058 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00488\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 27/100\n",
      "180/180 [==============================] - 8505s 47s/step - loss: 0.0058 - val_loss: 0.0049\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.00488 to 0.00485, saving model to weights-improvement-27-0.0049.hdf5\n",
      "Epoch 28/100\n",
      "  9/180 [>.............................] - ETA: 2:17:22 - loss: 0.0063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "reduce = ReduceLROnPlateau('val_loss', factor=0.1, patience=3, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce]\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator = image_a_b_gen(batch_size, Xtrain),\n",
    "    validation_data = image_a_b_gen(batch_size, Xvalid),\n",
    "    epochs = 100,\n",
    "    steps_per_epoch = ceil(len(Xtrain) / batch_size),\n",
    "    validation_steps = ceil(len(Xvalid) / batch_size),\n",
    "    callbacks = callbacks_list\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pour une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load black and white images\n",
    "color = []\n",
    "gray = []\n",
    "\n",
    "for filename in os.listdir('datasets/Images/'):\n",
    "    if filename.endswith('.jpg'):\n",
    "        image = cv2.imread('datasets/Images/'+filename)  \n",
    "        cv2.imshow('Original', image)\n",
    "        cv2.waitKey(0)\n",
    "        color.append(image)\n",
    "        break\n",
    "\n",
    "color = np.array(color)\n",
    "\n",
    "gray = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in color]\n",
    "gray = np.array(gray)\n",
    "gray = gray.reshape(gray.shape+(1,))\n",
    "\n",
    "X = gray / 255\n",
    "Y = color / 255\n",
    "        \n",
    "\n",
    "# Test model\n",
    "output = model.predict(X)\n",
    "output = np.array(output)\n",
    "\n",
    "#print(Y[0].shape)\n",
    "cv2.imshow('Gray', X[0])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#print('hi')\n",
    "print(output[0].shape)\n",
    "cv2.imshow('AI', output[0])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "WARNING:tensorflow:From C:\\Users\\ncharbit\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model\")\n",
    "model = load_model('model.h5')\n",
    "model.load_weights('weights-improvement-27-0.0049.hdf5')\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing OpenCV(cv2) module \n",
    "import cv2 \n",
    "  \n",
    "# Save image in set directory \n",
    "# Read RGB image \n",
    "image = cv2.imread('datasets/bleach-1585769.jpg')  \n",
    "print(image.shape)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "print(gray.shape)\n",
    "backtorgb = cv2.cvtColor(gray,cv2.COLOR_GRAY2RGB)\n",
    "print(backtorgb.shape)\n",
    "\n",
    "#cv2.imshow('Original image',image)\n",
    "#cv2.imshow('Gray image', gray)\n",
    "\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "\n",
    "    generator_input = Input(batch_shape=(None, 512, 512, 1), name='generator_input')\n",
    "    \n",
    "    conv1_32 = Conv2D(16,kernel_size=(3,3),strides=(1,1),padding='same',activation='elu',kernel_regularizer=l2(0.001))(generator_input)\n",
    "    conv1_32 = BatchNormalization()(conv1_32)\n",
    "    \n",
    "    conv2_64 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv1_32)\n",
    "    conv2_64 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv2_64)    \n",
    "    conv2_64 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv2_64)\n",
    "    conv2_64 = BatchNormalization()(conv2_64)\n",
    "    \n",
    "    conv3_128 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv2_64)\n",
    "    conv3_128 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv3_128)\n",
    "    conv3_128 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv3_128)\n",
    "    conv3_128 = BatchNormalization()(conv3_128)\n",
    "    \n",
    "    conv4_256 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv3_128)\n",
    "    conv4_256 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv4_256)\n",
    "    conv4_256 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv4_256)\n",
    "    conv4_256 = BatchNormalization()(conv4_256)\n",
    "    \n",
    "    conv5_512 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv4_256)\n",
    "    conv5_512 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv5_512 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv5_512)\n",
    "    conv5_512 = BatchNormalization()(conv5_512)\n",
    "    \n",
    "    conv6_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv6_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv6_512 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv6_512)\n",
    "    conv6_512 = BatchNormalization()(conv6_512)\n",
    "    \n",
    "    conv7_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv6_512)\n",
    "    conv7_512 = BatchNormalization()(conv7_512)\n",
    "    \n",
    "#     decoder\n",
    "    conv8_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv7_512)\n",
    "    conv8_512 = BatchNormalization(axis=1)(conv8_512)\n",
    "    \n",
    "    deconv9_512 = Conv2DTranspose(512,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(conv8_512)\n",
    "    deconv9_512 = BatchNormalization()(deconv9_512)\n",
    "    deconv9_512 = Concatenate()([deconv9_512,conv5_512])\n",
    "    deconv9_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv9_512)\n",
    "    deconv9_512 = BatchNormalization()(deconv9_512)\n",
    "    \n",
    "    deconv10_256 = Conv2DTranspose(256,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv9_512)\n",
    "    deconv10_256 = BatchNormalization()(deconv10_256)\n",
    "    deconv10_256 = Concatenate()([deconv10_256,conv4_256])\n",
    "    deconv10_256 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv10_256)\n",
    "    deconv10_256 = BatchNormalization()(deconv10_256)\n",
    "    \n",
    "    deconv11_128 = Conv2DTranspose(128,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv10_256)\n",
    "    deconv11_128 = Concatenate()([deconv11_128,conv3_128])\n",
    "    deconv11_128 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv11_128)\n",
    "    \n",
    "    deconv12_64 = Conv2DTranspose(64,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv11_128)\n",
    "    deconv12_64 = Concatenate()([deconv12_64,conv2_64])\n",
    "    deconv12_64 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv12_64)\n",
    "    \n",
    "    deconv13_32 = Conv2DTranspose(32,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv12_64)\n",
    "    deconv13_32 = Concatenate()([deconv13_32,conv1_32])\n",
    "    deconv13_32 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv13_32)\n",
    "    \n",
    "    deconv14_16 = Conv2DTranspose(16,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv13_32)\n",
    "    deconv14_16 = Conv2D(16,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv14_16)\n",
    "    \n",
    "    output = Conv2D(3,kernel_size=(1,1),padding='same',activation='relu')(deconv14_16)\n",
    "    \n",
    "    model = Model(inputs=generator_input,outputs=output)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
